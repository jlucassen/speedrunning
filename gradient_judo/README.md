# Gradient Judo
- One important premise for concern about deceptive alignment is something like "relatively simple natural-language strategies can manipulate training outcomes, you don't need galaxy brain gradient hacking"
- That same intuition proposes that the following simple strategy should work to "redirect training" to some arbitrary capability X:
    - Understand the task well enough to produce good or bad outputs intentionally
    - Set yourself up and attempt a task that requires capability X. Grade yourself. If you succeed, produce a good output. If you fail, produce a bad output.
- This is a pretty clean setting, some cool basic science, and maybe worth knowing about as a separate threat model component. I'm gonna see if I can demonstrate it.

## Split 1: can models self-grade?
- Target time: 1:00
- Start time:
- Notes:
- Stop time:
- Split time:
- Debrief notes: